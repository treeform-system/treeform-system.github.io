---
title: "Myfirstblog"
date: 2024-10-20T04:43:50-04:00
geekdocHidden: true
type: "posts"
author: "kevin"
draft: true
---

Over the years I always enjoyed writing programs from scratch with minimal dependencies as possible. I start my programming experience with python and quickly realized writing any program in python involved using some sort of black-box that called some other black-box to perform some function. Even python itself is just a C program under the hood. So I started digging deeper into languages like golang and C/C++. While I knew there were languages like Assembly that was what the hardware used directly it seemed to me that the most minimal and portable language was C. Although at the time I was not good enough at programming to enjoy writing in C, I stuck with other languages like golang which felt like a good middle ground to me. 

I have worked on dozens of projects over a wide range of fields some serious and some just for fun. And one of the things I found myself using the most often was a database. Having used SQLite so often I was curious how they actually worked under the hood. Although, I've often heard the phrase "don't build your own database" multiple times since for many this seems to be a solved problem where you leave the black-boxes to the experts. This is generally good advice, since databases seem to be one of the most reliable and well-standing pieces of software you will find. But, I was still curious how they actually worked under the hood.

## Why write a Database then?

This database will mainly be for academic pursuit of knowledge rather than an enterprise level solution. Over the course of time in making this database I have learned a lot of concepts and techniques that I can confidently say make me a better programmer, as well as exposing me to even more black boxes that most don't even realize. I dislike having to rely on black boxes but on some level all of computer science is just one abstraction over another and you have to make concessions on where you start using these abstractions. For many in python/javascript land importing 10K packages is an easy choice to do everything they could possibly think of. For those in C land defining every single byte of data is what they call fun. Golang seems to be a good middle ground between these two with its extensive standard library but also low level primitives access. I cannot say I am good enough to write my own thread scheduling policy, which is why I went with Golang since many of the primitives like Slices and Concurrency control structures are already defined for us. I am more interested slightly higher level structures like B+Tree, Wal, etc.

I've used databases before but never really thought about what happens when you do `Select * from MyTable;`. Or how my data actually gets stored when I insert a new row. On the surface it seems so simple and elegant to use, but I quickly found out the work needed to actually make this happen is quite intense.

## The realization of the work needed

Golang has a great package in the standard library for interfacing with a database in the **database/sql** package. This was a great starting point for me to start defining the actual functions on the other side of this. Now I didn't just start writing code though, I looked through some books and papers such as [Database internals](https://www.databass.dev/), [Desigining Data Intensive Applications](https://dataintensive.net/), and [Design and Implementation of Modern
Column-Oriented Database Systems](https://stratos.seas.harvard.edu/files/stratos/files/columnstoresfntdbs.pdf). Where I got a good idea for what I had to do and what problems I was trying to solve. There is even a nice reddit with helpful information for those wanting to get a good starting point as well (Reddit)[https://www.reddit.com/r/databasedevelopment/].

Although I had some planning beforehand writing out just the driver functions led to me having to write out the storage structure which led to me writing out the Parser structure because that's what the driver has to call and so on with many many other structures. Luckily I had written a Interpreter before based off of [this book](https://interpreterbook.com/), so writing the Parser was not too difficult. Even more luckily Golang has some great testing tools making writing test for each small structure less of a hassle than if I had chose to write this database in C like I originally planned to. 

This post is not meant to about the internals of the actual database engine implementation I actually detailed a bit about the internals on this website in the internals section. But for the most part this is how I tackled all the structures slowly and one by one learning new concepts like Bufferpools which I had never heard of before. It took a long time to fully learn so many different data structures such as WAL, b+tree (not to be confused with btree or b*tree), eviction polices (lru,etc.), etc. The main problem I faced was that many of these structures are not really talked about in a practical way. For the most part you'll find abstract blogs about good design or some very very specific documentation for a very specific design that isn't really applicable to many scenarios.

Reading the documentation for how to **use** a database is usually quite straigthforward since this is the part where the majority of people look at and therefore the part more focused on in technical writing. Reading the documentation for how a database is **made** is usually much harder, since there is just so much to cover it's difficult to detail out what they did and often it's best to just look at the source code to get an idea of what is going on. 

## So what is my database?

My database (whose name has changed many times because naming is stuff is hard) is meant to be a successor of SQLite. Originally, I intended to make a columnar storage database but there seems to be a quite modern and innovative database who also succeed SQLite called [duckdb](https://duckdb.org/). If I were to make a columnar database this is probably what I would strive for. So I had to pivot to a new idea, I know I said this was for academic purposes but I didn't want to just make a clone of something I wanted to put my own spin on it. 

SQLite was first released in 2000. Before I was even born. Database design and implementation has come a long way since then, one key point being a WAL. I won't talk too much about what a WAL is since I'm sure there is much better explanations out there. But for short summary it mainly just keeps transactions safe once committed by having a log of everything that is happening that can quickly be written to and therefore checked afterwards to keep the database state safe. SQLite originally used something called a Rollback Journal and later on incorporated a WAL into it. Although many people don't even know this since it's one of those things you have to opt into since the Rollback Journal is the default. SQLite main selling point is that it's one file because back then filesystems were much more fragile and you will find many database engines actually built their own virtual filesystem to not have to rely on the OS(operating system). Being just one file has it's advantages, but for mine I decided to have each table on it's own file and take advantage of Golang fantastic concurreny control, giving users more thread safety/speed by reducing a potential bottleneck. 

My database is meant to be a successor of SQLite in the sense that it is also an in-process database as well as oriented for OLTP purposes. OLAP databases (like duckdb) are great for crunching numbers on large datasets, but OLTP databases are meant for more server applications where you're mainly retrieving data from the database. My database also follows the Relational design as well as ACID (Atomicity, Consistency, Isolation, and Durability). Databases like Postgres or MYSQL are great for businesses where you have lots and lots of data and you need to make sure it is safe and easy to access, but this comes at a cost. These databases are actually DBMS (database management system) and run as their own process and for the most part require a seperate server in a production environment. But if you want to just test something simple having to configure these DBMS can be quite a pain which is where being able to use something like SQLite really shines since it is so easy to just spin up and get running within your application code since it is just a file. Given that SQLite and my database are both in-process it is trivial to instantiate a new database and store it next to your application, which for many people is often a good enough solution. SQLite has been shown to handle enterprise level of service, and this kind of reliability is something I will be aiming for as well. No one wants to configure yet another server, and being able to quickly spin up a program with a database is often what developers want. My database is meant to be that, a good/reliable program someone making a simple program in golang can use to store their data when they don't need an enterprise level solution of backups and speed because for most applications SQLite is good enough and mine is there when you need just a little bit more than SQLite.

## Did I mention this project is in pure Golang with zero dependencies?

That's right **no** dependencies only the standard library and golang. Although if you look at the go.mod file you will see dependencies but these are just for testing purposes and therefore the actual database code does not rely on these at all and someone using this database won't need them either. This kind of shows the extensiveness of the Golang standard library giving you the ability to do so much right out of the box.